---
title: "Jack Hales AI Agents Research Journal - 2024"
description: "AI Agent thoughts and research from Jack Hales within 2024."
date: "17 November 2024"
---

**Preface:** I want to define the minimum high-impact activities an agent can do within my workflow. Then, I want to evaluate the potential benefits of creating a permanent runtime, rather than a temporary "invoked" runtime.

### What is an AI Agent?

My conception of an AI Agent is something that runs in a temporary "invoked" runtime, or a permanent runtime, within a specific structure of a LLM-model. It involves inputs, as well as an attempt to coerce to a model-decided output, and an output parser to run certain actions based on the models output.

The inputs conventionally involve a mix of a chaotic user-request, as well as common instructions for how to respond. Something like: 

> "You will respond to user input. You will return output in JSON. Your options are ---. The users input is: ---."

With a bit more rigidity - but the basic idea is encapsulated above.

### Is an AI Agent even necessary?

This is an important aspect to review, before jumping into AI Agent world. To help, I will start with what I commonly do at work, as well as some unusual exceptions.

- **Programming:** I am often taking feature ideas or bug fixes, and converting them into the code which is required to resolve the given task.
- **Emails:** I read and respond to a plethora of emails - some business-important, some functional, some documentative.
- **Testing:** I am often trying to solve data pipelines by doing testing on components.
- **Calling:** I am often calling individuals, to help install software, or diagnose problems in systems. <span style={{color: "red"}}>Not AI-Agent capable - within this current scope of initial review.</span>
- **Meeting:** I meet with other individuals. The only axis of adjusting this, is to change requirements, and redirect meetings to emails, calls, or texts - as Naval Ravikant refers to. <span style={{color: "red"}}>Not AI-Agent capable.</span>

The above does not define with enough definition, what a given task involves. This has often been my problem with using AI - it's a series of semi-random occurrences.

I also don't think I will be able to "plan" the perfect agent. It'll have to evolve - naturally - by using it in the field. I will make a conscious effort to test the ideas, rather than just discuss them.

### What LLMs might be natively great for

LLMs are text parsers, and text-continuers. OpenAI has done a good job of separating the model, as well as the asker, so the models job is to step forward one more model response, then wait for user feedback.

Due to relying on training data, models think inside their training data - they think "in their box".

Ask an LLM to:

- Tell you what you can make with existing ingredients: It will do well and provide you existing recipies. It will not risk creating a new recipie.
- Ask an LLM to refactor or assess some code: It will do it within existing principles, unless directed to do something else it can understand.
- Ask an LLM to make a decision: It will consider in-box options.
- Ask an LLM to classify some input based on a series of categories: It will do it as it believes.
- Ask an LLM to simulate being something: It will do as it believes, mimicing what it believes that thing is.
- Ask an LLM to adjust prose, mimic a style: It will do as it believes.

These are high-level claims, and I hope you understand where I am coming from: **Models think inside their box**. This is OKAY! That is what they are made to do!

(PLEASE NOTE: I am writing a lot of fluff here. The above is to be revised after I do some more work.)

## 23 November 2024

I've travelled north from my place to visit family, on the way enjoying the recent [5-hour Lex Fridman interview](https://www.youtube.com/watch?v=ugvHCXCOmm4) with [Dario Amodei](https://darioamodei.com/), [Christopher Olah](https://colah.github.io/about.html) and [Amanda Askell](https://askell.io/) - three deeply interesting guests. Dario is CEO, Christopher is working on understanding the internal circuits of model brains, and Amanda is a Philosophy-Engineer focussing on alignment, system prompts, and chatting with Anthropic's Claude.

- One focus of Anthropic is safety. Dario believes in a "race to the top" approach, positive-sum.
- Move fast, with safety backed by testing frameworks. Using Evals, as well as their RSP/ASL levels, they can use progressive testing.
- Philosophy, in Amanda's work pertaining to prompting and neutralising edges of Claude, has served strongly.
- Chris' work on Mechanistic Interpretation is very fascinating, as a relatively small emerging field.
- Anthropic's Constitutional AI, and Synthetic data generation, are poised as potential ways to increase training data.

Their work is excellent, and the conversation was very enlightening as to how these LLM/AI companies operate and think.

### ASL

ASL (AI Safety Levels), is a framework to quantify model broad-scale risk. It aims to distinguish risk levels, to justify more resource investment into ensuring safety is met. Since intelligence seems to increase when models run longer, work in workflows, or agents, I'd hope that companies are testing these as well as one-shot responses - which I'm sure they are!

- ASL1 = relatively narrow-scoped AI, like DeepMind.
- ASL2 = wider-scoped more general AI, capability of providing malicious instructions, misleading, limited autonomy.
- ASL3 = ability to act, reflection, self-optimise (self-train).
- ASL4 = self-adapt, complex workflows, potential emergent behaviours.
- ASL5 = ...

(Simplified).

