---
title: "Jack Hales AI Agents Research Journal - 2024"
description: "AI Agent thoughts and research from Jack Hales within 2024."
date: "17 November 2024"
---

**Preface:** I want to define the minimum high-impact activities an agent can do within my workflow. Then, I want to evaluate the potential benefits of creating a permanent runtime, rather than a temporary "invoked" runtime.

### What is an AI Agent?

My conception of an AI Agent is something that runs in a temporary "invoked" runtime, or a permanent runtime, within a specific structure of a LLM-model. It involves inputs, as well as an attempt to coerce to a model-decided output, and an output parser to run certain actions based on the models output.

The inputs conventionally involve a mix of a chaotic user-request, as well as common instructions for how to respond. Something like: 

> "You will respond to user input. You will return output in JSON. Your options are ---. The users input is: ---."

With a bit more rigidity - but the basic idea is encapsulated above.

### Is an AI Agent even necessary?

This is an important aspect to review, before jumping into AI Agent world. To help, I will start with what I commonly do at work, as well as some unusual exceptions.

- **Programming:** I am often taking feature ideas or bug fixes, and converting them into the code which is required to resolve the given task.
- **Emails:** I read and respond to a plethora of emails - some business-important, some functional, some documentative.
- **Testing:** I am often trying to solve data pipelines by doing testing on components.
- **Calling:** I am often calling individuals, to help install software, or diagnose problems in systems. <span style={{color: "red"}}>Not AI-Agent capable - within this current scope of initial review.</span>
- **Meeting:** I meet with other individuals. The only axis of adjusting this, is to change requirements, and redirect meetings to emails, calls, or texts - as Naval Ravikant refers to. <span style={{color: "red"}}>Not AI-Agent capable.</span>

The above does not define with enough definition, what a given task involves. This has often been my problem with using AI - it's a series of semi-random occurrences.

I also don't think I will be able to "plan" the perfect agent. It'll have to evolve - naturally - by using it in the field. I will make a conscious effort to test the ideas, rather than just discuss them.

### What LLMs might be natively great for

LLMs are text parsers, and text-continuers. OpenAI has done a good job of separating the model, as well as the asker, so the models job is to step forward one more model response, then wait for user feedback.

Due to relying on training data, models think inside their training data - they think "in their box".

Ask an LLM to:

- Tell you what you can make with existing ingredients: It will do well and provide you existing recipies. It will not risk creating a new recipie.
- Ask an LLM to refactor or assess some code: It will do it within existing principles, unless directed to do something else it can understand.
- Ask an LLM to make a decision: It will consider in-box options.
- Ask an LLM to classify some input based on a series of categories: It will do it as it believes.
- Ask an LLM to simulate being something: It will do as it believes, mimicing what it believes that thing is.
- Ask an LLM to adjust prose, mimic a style: It will do as it believes.

These are high-level claims, and I hope you understand where I am coming from: **Models think inside their box**. This is OKAY! That is what they are made to do!

(PLEASE NOTE: I am writing a lot of fluff here. The above is to be revised after I do some more work.)

### Let's Build

