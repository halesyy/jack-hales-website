---
title: "Jack Hales Prime Number Research Journal - 2024"
description: "A brief overview of the renewed research I've been doing on prime numbers in 2024."
date: "23 October 2024"
---

## 23rd of October, 2024

I've taken a renewed interest in this prime number research project for a perplexity of reasons. I read and re-read a lot of Nassim Taleb, and a few of his paraphrased points have been of great influence. Specifically:

**Black Swan, the middle of Chapter 11, discussing the 3-body problem:** Forecasting accrues tiny compounding errors, leading to large deviations.

This leads into chaos theory, the work of Pointcarre, chaos theory and topology.

Taleb has also discussed that tinkering gives him a higher-IQ than planning to solve problems.

**Finding bird poop:** Researchers looking into the universe picked up hums of old radiation from the birth of the universe. They assumed it was bird poop from radar dishes, instead they were shocked with their accidental discovery.

This all comes to provide me with accidental validation that the prime number generator I am working on, and the way I was going about it, was not so bad at all. Further, it might lead to some more tools for other practitioners on handling chaos and tinkering.

To recap what I am experimenting with: Denoting an equation which predicts the next prime number.

**Chaos theory, prediction application:** This is a unique application where the chaos of sequential prime numbers is very volatile and maddening, but the accuracy of the sample data, and ability to generate it - is logical and functional.

**Confirmation carefulness:** In my previous word, confirmation was the killer. You could have a very well-fit equation at primes 1-1,000,000, and then lose it all in the next million sequential primes.

**Manufactured tinkering:** I am not that smart with mathematics besides the basics, so I wrote a brute force equation builder. I then created a "fitness" algorithm to rank them. This provided me with very telling equations.

**P=NP:** Although there is no direct link to Nth-prime, and NP complexity - there is a growing amount of computational complexity involved in finding new primes. It's also crossed my mind that these "secondary equation problems" could exist, where algorithmic problems can be solved mathematically. I have no backup for this idea!

Now that I've preambled, I am going to update this with my findings as I find them - like a journal - to assist in the valuable art of seeing events as they occur.

### Creating Equations

Creating equations is an important piece of the puzzle. My first algorithm was very inefficient, and could take hours to find valuable equations by brute-forcing. My aim is to play around with building a quicker algorithm generator by understanding the form of equations deeper.

I'm also concerned that there might be logical functions that are needed that do not exist in existing mathematics which are required to express the logical pattern of primes. I cannot determine this, but if needed, I will be able to bake in custom functions into this interpreter.

- [This](https://github.com/halesyy/brute-primes/tree/main) is where I left off in April last year, and I did have an interesting idea - encoding equations into integer or float matrixes, so generating could done much quicker.
- The messy initial approach to generating equations, which I did in [my first approach in Jan 2022](https://github.com/halesyy/lol-primes/blob/main/solver/iter.py#L59), is to load in an array of possible equation parts, and generate N parts to try to successfully get an equation. Test it. If it passes, it can be used. This leads to *some* valid equations, but most aren't.
- I'm going to have a go at building an array-based interpreter which can then construct into an expression. I am skeptical, but it might yield some better and more efficient analysis.

The above are 3 approaches to creating equations.

#### Creating Equations - Numeric

The first dot-point was never integrated, but I believe it is a good quickly approach. The aim is to embed the discussed expression tokens (cos, sin, (, ), etc) into a matrix. One difficulty I had with this was the embedding of floats into an equation, which I think can be **solved with types**. Python distinguishes between integers and floats, and we can distinguish `1.00` from `1`. This way, we can leave ints for expression tokens, and floats for equation float-representations.

[My first successfull (messy) implementation](https://github.com/halesyy/lol-primes/blob/main/solver/iter.py#L159) walked through an algorithm like:

1. Generate an equation length from 1-100.
2. Make a strand (input: length): this is a series of letters (a, s, c, g) which correspond to **attachment**, **substitutable**, **constant** or **glue**.
3. Make an equation by supplementing the letters for their possible values (glue = +-*/, etc).
4. Check the equation with a sample response to see if it's valid.

This worked well, and one assumption I am realising now is that it always started with an "attachment" to start. This was X, PI or E.

Upon implementing a very basic version to get started, raw compute suffices to build some successful equations. See the code [here](https://github.com/halesyy/prime-number-research/blob/063619a936f005daa69d203464aee21f6ba33562/primes/expressions/generator.py#L104).

I will review this component in the future, once I have a working end-to-end fitter.

### Fitting to primes

To fit the values, I have created a separate function which takes in the results of the expression, as well as the primes it is to be tested against. I'm using a cumulative absolute difference, to test against the best values. I've written this out, and now I'm going to the shops and will leave this running during that time to see what it can produce. This is **the most inefficient version of what I am attempting to create** using the new version, to test the approach. [See here](https://github.com/halesyy/prime-number-research/blob/b053f3cd8d001def0ac8cd5572f4d7d4cd9bddf5/primes/fitter.py#L78).

### Properties

**Properties** are important, as they define what we're looking for in an equation. A straightforward one for prime numbers is:

- **N+1 prime number > N prime number** - every prime is larger than the last.

This leads to an optimisation in the result checker, which would normally have to check over N primes we're testing against.

### Reminder: The best we've got from attempt 1 and 2

The best equation I was able to fit was:

> **x*log(x,y)** (y=7 to start, x steps by 2) - tested below further

Also:

> **e^cos(x) + e*x** 

These are interesting prime-fitting equations.

## 24th of October, 2024

It's clear the new modified numeric equation builder is slow. We know from the previous iteration that the above equations can exist. They would be represented as an array as such:

```py
[
   3.00, # 3.00 float
   2, # *
   14 # log(x, y)
]
```

Conditions:

- A randint of 3 (1% chance)
- y=7 (we have a range that steps from -10 to 10 for y values)

Creates:

- `x*log(x,y)`

When inputting this manually, we get a quite bad fitness level when we're stepping x=1,2,3, ...

In fact, y=7 is meant to be the best fitness of this equation. But we do not see that at all.

At a x step of 1, we see the best fitness is at y=3. The fitness being `1,185,775` (absolute cumulative difference to the first 1,229 primes).

What I recall from the previous project, is that there is a weird difference when we step by x=2. When we apply it here, we get what we were seeing in the past. At x=2,4,6,8,10,..., we get the best fitness at y=7, and a cumulative absolute difference of `65,995`.

When we narrow down from 6-8, stepping by 0.001, we get the following fitness plot:

![Y values over x*log(x,y)](/y_vs_fitness.png)

The minimum in the test is y=6.84, at a fitness of `23,710`.

For your interest, here is a figure of the equation alongside the prime numbers:

![Primes versus best-fit x*log(x,y)](/primes_vs_best_fit.png)

It's strikingly well-fit at this size.

And, as I am sure you are wondering, here is the difference between these - to get a slightly closer look into one of prime numbers radically beautiful visualisations:

![Difference in prediction and primes of x*log(x,y)](/difference.png)

Technically, the above is all of our model errors. It becomes quickly clear why this problem has complexity baked into it - to somehow quantify this complexity into an equation. This relates back to the discussion on chaos theory (which I have studdied nothing on), and is where this article starts to touch area I've not been at before. The above are errors that accrue over time, so let's look at how they span over 1,000,000 primes:

![Difference in prediction and primes of x*log(x,y) for 1 million primes](/difference_1000000.png)

Remember that **this is still with our y=6.84** value we found through fitness. If we go back and try to fit a best y value with our million smaple-set, we get a new fitness value: *y=56* brings us to a error rate of:

![Difference in prediction and primes with a new fitness](/difference_fitted_1000000.png)

Going down this rabbit hole, similar to plotting the minimum value of y-values above, moves us into **overfitting territory**. What strikes me (and I don't want to get narrowed down on this idea), is that the new goal should instead be as follows:

> **Attempt to flatten this curve at large N.** If there's a way to flatten this, and get differences closer to 0 as N rises higher and higher, then the model difference is a more attackable problem.

From here, as I've fixed the equation modeller also, I am going to test some more brute-forcing equations to see if I can find anything else to help this adventure.

### Equation Modelling

My knack is that our equations are platonic (Talebian term) in a sense that we're modelling in a standard logical language, something of which might require complex, or non-platonic randomness and chaos. This is just a hunch - but at the same time I am really shocked at how well a basic equation can model the randomness of prime numbers. I do not understand logarithms in my soul, only on the surface, so this is an area I am going to look further into.

### Equation Interfacing 

There is an existing complexity which I accidentally noticed - testing at x=2,4,6 rather than x=1,2,3 is inherrently weird - but worked well. It's possible that a rate change in the x increment is a meta-equation which can reduce complexity in the equation itself, and also lends itself to a higher rate of experimentation.

I'm considering, though find it hard to grasp, building a more dynamic interface which is able to accomodate this. At the moment, I am using the following lines to for example bake in a 2,4,6 step in x values:

```py
X_INC = 1
X_TIMES = 2
...
for x, _ in enumerate(primes):
   x = x + X_INC
   x = x * X_TIMES
```

By creating an equation to build X values (2x, x^2, or otherwise), this starts to give light to the idea of combining multiple series of equations to solve the problem. This might be unnecessary, as this can also be represented as:

> (2 * x) * (log((x*2), 7)) at x-step = 1

Which provides the same output as stepping by 2. By being able to accomodate this structure (which cannot currently), this can be tested for efficacy before adding this complexity.

I've done this by adding another set of options for the expression builder with:

```py
ERROROUS_ADDITIONS = [
   "log", # 18
   "sin",
   "cos",
   "," # 21
]
```

Which can embed the above function on x-step=1 by embedding the following:

```py
ex_str = supplement_ops(
   [6, 2.00, 2, 10, 7] + # (2x)
   [2] + # \*
   [18, 6, 2.00, 2, 10, 21, 11, 7] 
) # returns: "(2.0*x)*log(2.0*x,y)"
```

The question then becomes:

> Is the problem space better if fractionated into multiple equations, or is it better to attempt to build a mono equation?

And I have no idea.

When running this modified generator with **more possible equations**, and also **a higher chance of a failed equation**, some equations and their 1,299 fitness are:

- Cut down length of equations from 100 to 25, since most equations so far have fit there. I will lift this in the future to not be fooled by randomness.
- **`((x*sin(sin(1.1717823427685636)))*log(x,y)) at y=2:`** 17,323 fitness (great one-shot result before optimisation)

### ((x*sin(sin(1.1717823427685636)))*log(x,y))

Taking this equation, and y-fitting, we find it gets most comfortable at `y=2.0026` (14533.53 fitness). Narrowing down further, we get a miniscule smaller at `y=2.002564` (14533.16 fitness). Tiny difference in a higher precision.

At 1,000,000 primes, we have the following fitnesses:

- **Prior x*log equation fitness:** 811,473,649
- **New sinusoudal fitness:** 511,239,573

New sinusoidal plotted:

![Sinusoidal equation plotted](/primes_vs_best_fit_sinusoidal.png)

And difference:

![Sinusoidal difference](/difference_sinusoidal.png)

As you can see - the difference is negative (grows larger than the primes at scale).

What I feel is a property for a prime number expression or equation, is something like:

> It cannot be fitted to a certain area (first 1,229), and then expect that fitness to scale further to a larger area.

It's also noticeable the constant in this equation: `1.1717823427685636`. This is actually a constant inside a sin(sin(...)) function, which then resolves this constant to `0.7964759083371821`. The simpler equation is then `((x*0.7964759083371821)*log(x,y))`, which brings us right back to the start.

**I was fooled by randomness - and tokens.**

(I was going to remove this section, but I am committing to the format of a journal, to outline the process and how it felt to move from each step to another.)

This is bitter-sweet - and let me explain why.

### Multivariate

I was considering whether or not multivariate (beyond X and Y) could help in the complexity of primes. In my handwritten journal, I wrote the following note:

> (With the broader equation builder, which can facilitate 2*x and log((2*x), y) for example) we have a broader dynamic chance. For example: we can multiple x by 2 different equations, OR constants. They are both variables.

What I meant by this was that with the broader equation builder - although it operates with a much larger error change, the "possible" space is larger. Like monkeys typing randomly to make shakespeare - if you restrict the alphabet or wordlist, you might miss dickinson.

This is an immediate validation that there is more possible variation, since we've created a multivariate example within ~15 minutes of running this random generator - which is *more* fit to the prime numbers than previous models.

I think this comes together well with the above next steps in my mind.

### Optimising the Constant (C)

When optimising the constant, it can be brought down with the following slight digit changes:

![Constant tweaking results](/constant_tweaking.png)

This creates the difference from the primes of the following:

![Constant fitting best difference](/difference_best_fit_constant.png)

### Next Steps - Optimising, and Grouping Primes

Next ideas:

- It may be valuable to tweak this `0.7964759083371821` constant in the modified equation WITHIN the groups, to find how this adjusts.
- It may also be valuable, once this is worked out, to check at the optimal fitness as the groups scale. We can break up the 1,000,000 primes into groups of 1,000 groups of 1,000.
- Modelling and plotting these might provide an interesting scale that can be seen - or some randomness that cannot be tamed. We'll see!

With optimising, there is a possibility that X, or other values, might need to be adjusted across a separate equation. First, I will try adjusting the separate constant.

### Nearness

Something I have noted when looking at the difference charts, is that **there seems to be some form of nearness bias, or gravity**. What I mean by this is that no subsequent "draw from the bag" for the next difference value is radically different from the previous difference. In fact, they seem to cluster, and follow trends, or mean-reversion. When I take the second-delta of the difference (delta of the difference), we see this chart:

![Delta of the difference values](/difference_delta.png)

This displays two properties:

- Positive values could skew further and further into infinity.
- Negative values seem to follow a inverse log asymptote (if that makes sense).

If we then use log scales for the Y axis of the delta difference for the first 1,000,000 primes, we see:

![1,000,000 log Y difference delta](/difference_delta_log_y)

And zooming into the first 250 WITHOUT log of the above series, we see:

![First 250 primes NO LOG difference delta](/250_primes_difference_delta_nolog.png)

(For this one, I recommend right-clicking, and open in another tab, as it's better to see it on a larger device.)

With LogY:

![First 250 primes log Y difference delta](/250_primes_difference_delta_log_y.png)

Both show interesting patterns, though with LogY it seems fuzzier locally, whereas at scale, it seems as if there's a bias towards negativity. The problem then might simplify into: trying to balance positive difference and negative difference at scale, which we might solve with tweaking in groups.

For the nolog plot, the bottom line of the pattern looks like a plot, as well as the band above it looks like it follows with constant fixture.

I want to remind myself and readers that **it is not my job to reason about this crazy randomness** to solve the problem - because I cannot plan the answer in any way. It's too wild. So, I will keep stepping forward cutting up little problems into heuristics, and trying to reason about the properties of what is occurring here.