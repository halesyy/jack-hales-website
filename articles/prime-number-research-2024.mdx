---
title: "Jack Hales Prime Number Research Journal - 2024"
description: "A brief overview of the renewed research I've been doing on prime numbers in 2024."
date: "23 October 2024"
---

## 23rd of October, 2024

I've taken a renewed interest in this prime number research project for a perplexity of reasons. I read and re-read a lot of Nassim Taleb, and a few of his paraphrased points have been of great influence. Specifically:

**Black Swan, the middle of Chapter 11, discussing the 3-body problem:** Forecasting accrues tiny compounding errors, leading to large deviations.

This leads into chaos theory, the work of Pointcarre, chaos theory and topology.

Taleb has also discussed that tinkering gives him a higher-IQ than planning to solve problems.

**Finding bird poop:** Researchers looking into the universe picked up hums of old radiation from the birth of the universe. They assumed it was bird poop from radar dishes, instead they were shocked with their accidental discovery.

This all comes to provide me with accidental validation that the prime number generator I am working on, and the way I was going about it, was not so bad at all. Further, it might lead to some more tools for other practitioners on handling chaos and tinkering.

To recap what I am experimenting with: Denoting an equation which predicts the next prime number.

**Chaos theory, prediction application:** This is a unique application where the chaos of sequential prime numbers is very volatile and maddening, but the accuracy of the sample data, and ability to generate it - is logical and functional.

**Confirmation carefulness:** In my previous word, confirmation was the killer. You could have a very well-fit equation at primes 1-1,000,000, and then lose it all in the next million sequential primes.

**Manufactured tinkering:** I am not that smart with mathematics besides the basics, so I wrote a brute force equation builder. I then created a "fitness" algorithm to rank them. This provided me with very telling equations.

**P=NP:** Although there is no direct link to Nth-prime, and NP complexity - there is a growing amount of computational complexity involved in finding new primes. It's also crossed my mind that these "secondary equation problems" could exist, where algorithmic problems can be solved mathematically. I have no backup for this idea!

Now that I've preambled, I am going to update this with my findings as I find them - like a journal - to assist in the valuable art of seeing events as they occur.

### Creating Equations

Creating equations is an important piece of the puzzle. My first algorithm was very inefficient, and could take hours to find valuable equations by brute-forcing. My aim is to play around with building a quicker algorithm generator by understanding the form of equations deeper.

I'm also concerned that there might be logical functions that are needed that do not exist in existing mathematics which are required to express the logical pattern of primes. I cannot determine this, but if needed, I will be able to bake in custom functions into this interpreter.

- [This](https://github.com/halesyy/brute-primes/tree/main) is where I left off in April last year, and I did have an interesting idea - encoding equations into integer or float matrixes, so generating could done much quicker.
- The messy initial approach to generating equations, which I did in [my first approach in Jan 2022](https://github.com/halesyy/lol-primes/blob/main/solver/iter.py#L59), is to load in an array of possible equation parts, and generate N parts to try to successfully get an equation. Test it. If it passes, it can be used. This leads to *some* valid equations, but most aren't.
- I'm going to have a go at building an array-based interpreter which can then construct into an expression. I am skeptical, but it might yield some better and more efficient analysis.

The above are 3 approaches to creating equations.

#### Creating Equations - Numeric

The first dot-point was never integrated, but I believe it is a good quickly approach. The aim is to embed the discussed expression tokens (cos, sin, (, ), etc) into a matrix. One difficulty I had with this was the embedding of floats into an equation, which I think can be **solved with types**. Python distinguishes between integers and floats, and we can distinguish `1.00` from `1`. This way, we can leave ints for expression tokens, and floats for equation float-representations.

[My first successfull (messy) implementation](https://github.com/halesyy/lol-primes/blob/main/solver/iter.py#L159) walked through an algorithm like:

1. Generate an equation length from 1-100.
2. Make a strand (input: length): this is a series of letters (a, s, c, g) which correspond to **attachment**, **substitutable**, **constant** or **glue**.
3. Make an equation by supplementing the letters for their possible values (glue = +-*/, etc).
4. Check the equation with a sample response to see if it's valid.

This worked well, and one assumption I am realising now is that it always started with an "attachment" to start. This was X, PI or E.

Upon implementing a very basic version to get started, raw compute suffices to build some successful equations. See the code [here](https://github.com/halesyy/prime-number-research/blob/063619a936f005daa69d203464aee21f6ba33562/primes/expressions/generator.py#L104).

I will review this component in the future, once I have a working end-to-end fitter.

### Fitting to primes

To fit the values, I have created a separate function which takes in the results of the expression, as well as the primes it is to be tested against. I'm using a cumulative absolute difference, to test against the best values. I've written this out, and now I'm going to the shops and will leave this running during that time to see what it can produce. This is **the most inefficient version of what I am attempting to create** using the new version, to test the approach. [See here](https://github.com/halesyy/prime-number-research/blob/b053f3cd8d001def0ac8cd5572f4d7d4cd9bddf5/primes/fitter.py#L78).

### Properties

**Properties** are important, as they define what we're looking for in an equation. A straightforward one for prime numbers is:

- **N+1 prime number > N prime number** - every prime is larger than the last.

This leads to an optimisation in the result checker, which would normally have to check over N primes we're testing against.

### Reminder: The best we've got from attempt 1 and 2

The best equation I was able to fit was:

> **x*log(x,y)** (y=7 to start, x steps by 2) - tested below further

Also:

> **e^cos(x) + e*x** 

These are interesting prime-fitting equations.

## 24th of October, 2024

It's clear the new modified numeric equation builder is slow. We know from the previous iteration that the above equations can exist. They would be represented as an array as such:

```py
[
   3.00, # 3.00 float
   2, # *
   14 # log(x, y)
]
```

Conditions:

- A randint of 3 (1% chance)
- y=7 (we have a range that steps from -10 to 10 for y values)

Creates:

- `x*log(x,y)`

When inputting this manually, we get a quite bad fitness level when we're stepping x=1,2,3, ...

In fact, y=7 is meant to be the best fitness of this equation. But we do not see that at all.

At a x step of 1, we see the best fitness is at y=3. The fitness being `1,185,775` (absolute cumulative difference to the first 1,229 primes).

What I recall from the previous project, is that there is a weird difference when we step by x=2. When we apply it here, we get what we were seeing in the past. At x=2,4,6,8,10,..., we get the best fitness at y=7, and a cumulative absolute difference of `65,995`.

When we narrow down from 6-8, stepping by 0.001, we get the following fitness plot:

![Y values over x*log(x,y)](/y_vs_fitness.png)

The minimum in the test is y=6.84, at a fitness of `23,710`.

For your interest, here is a figure of the equation alongside the prime numbers:

![Primes versus best-fit x*log(x,y)](/primes_vs_best_fit.png)

It's strikingly well-fit at this size.

And, as I am sure you are wondering, here is the difference between these - to get a slightly closer look into one of prime numbers radically beautiful visualisations:

![Difference in prediction and primes of x*log(x,y)](/difference.png)

Technically, the above is all of our model errors. It becomes quickly clear why this problem has complexity baked into it - to somehow quantify this complexity into an equation. This relates back to the discussion on chaos theory (which I have studdied nothing on), and is where this article starts to touch area I've not been at before. The above are errors that accrue over time, so let's look at how they span over 1,000,000 primes:

![Difference in prediction and primes of x*log(x,y) for 1 million primes](/difference_1000000.png)

Remember that **this is still with our y=6.84** value we found through fitness. If we go back and try to fit a best y value with our million smaple-set, we get a new fitness value: *y=56* brings us to a error rate of:

![Difference in prediction and primes with a new fitness](/difference_fitted_1000000.png)

Going down this rabbit hole, similar to plotting the minimum value of y-values above, moves us into **overfitting territory**. What strikes me (and I don't want to get narrowed down on this idea), is that the new goal should instead be as follows:

> **Attempt to flatten this curve at large N.** If there's a way to flatten this, and get differences closer to 0 as N rises higher and higher, then the model difference is a more attackable problem.

From here, as I've fixed the equation modeller also, I am going to test some more brute-forcing equations to see if I can find anything else to help this adventure.

### Equation Modelling

My knack is that our equations are platonic (Talebian term) in a sense that we're modelling in a standard logical language, something of which might require complex, or non-platonic randomness and chaos. This is just a hunch - but at the same time I am really shocked at how well a basic equation can model the randomness of prime numbers. I do not understand logarithms in my soul, only on the surface, so this is an area I am going to look further into.

### Equation Interfacing 

There is an existing complexity which I accidentally noticed - testing at x=2,4,6 rather than x=1,2,3 is inherrently weird - but worked well. It's possible that a rate change in the x increment is a meta-equation which can reduce complexity in the equation itself, and also lends itself to a higher rate of experimentation.

I'm considering, though find it hard to grasp, building a more dynamic interface which is able to accomodate this. At the moment, I am using the following lines to for example bake in a 2,4,6 step in x values:

```py
X_INC = 1
X_TIMES = 2
...
for x, _ in enumerate(primes):
   x = x + X_INC
   x = x * X_TIMES
```

By creating an equation to build X values (2x, x^2, or otherwise), this starts to give light to the idea of combining multiple series of equations to solve the problem. This might be unnecessary, as this can also be represented as:

> (2 * x) * (log((x*2), 7)) at x-step = 1

Which provides the same output as stepping by 2. By being able to accomodate this structure (which cannot currently), this can be tested for efficacy before adding this complexity.

I've done this by adding another set of options for the expression builder with:

```py
ERROROUS_ADDITIONS = [
   "log", # 18
   "sin",
   "cos",
   "," # 21
]
```

Which can embed the above function on x-step=1 by embedding the following:

```py
ex_str = supplement_ops(
   [6, 2.00, 2, 10, 7] + # (2x)
   [2] + # \*
   [18, 6, 2.00, 2, 10, 21, 11, 7] 
) # returns: "(2.0*x)*log(2.0*x,y)"
```

The question then becomes:

> Is the problem space better if fractionated into multiple equations, or is it better to attempt to build a mono equation?

And I have no idea.

When running this modified generator with **more possible equations**, and also **a higher chance of a failed equation**, some equations and their 1,299 fitness are:

- Cut down length of equations from 100 to 25, since most equations so far have fit there. I will lift this in the future to not be fooled by randomness.
- **`((x*sin(sin(1.1717823427685636)))*log(x,y)) at y=2:`** 17,323 fitness (great one-shot result before optimisation)

### ((x*sin(sin(1.1717823427685636)))*log(x,y))

Taking this equation, and y-fitting, we find it gets most comfortable at `y=2.0026` (14533.53 fitness). Narrowing down further, we get a miniscule smaller at `y=2.002564` (14533.16 fitness). Tiny difference in a higher precision.

At 1,000,000 primes, we have the following fitnesses:

- **Prior x*log equation fitness:** 811,473,649
- **New sinusoudal fitness:** 511,239,573

New sinusoidal plotted:

![Sinusoidal equation plotted](/primes_vs_best_fit_sinusoidal.png)

And difference:

![Sinusoidal difference](/difference_sinusoidal.png)

As you can see - the difference is negative (grows larger than the primes at scale).

What I feel is a property for a prime number expression or equation, is something like:

> It cannot be fitted to a certain area (first 1,229), and then expect that fitness to scale further to a larger area.

It's also noticeable the constant in this equation: `1.1717823427685636`. This is actually a constant inside a sin(sin(...)) function, which then resolves this constant to `0.7964759083371821`. The simpler equation is then `((x*0.7964759083371821)*log(x,y))`, which brings us right back to the start.

**I was fooled by randomness - and tokens.**

(I was going to remove this section, but I am committing to the format of a journal, to outline the process and how it felt to move from each step to another.)

This is bitter-sweet - and let me explain why.

### Multivariate

I was considering whether or not multivariate (beyond X and Y) could help in the complexity of primes. In my handwritten journal, I wrote the following note:

> (With the broader equation builder, which can facilitate 2*x and log((2*x), y) for example) we have a broader dynamic chance. For example: we can multiple x by 2 different equations, OR constants. They are both variables.

What I meant by this was that with the broader equation builder - although it operates with a much larger error change, the "possible" space is larger. Like monkeys typing randomly to make shakespeare - if you restrict the alphabet or wordlist, you might miss dickinson.

This is an immediate validation that there is more possible variation, since we've created a multivariate example within ~15 minutes of running this random generator - which is *more* fit to the prime numbers than previous models.

I think this comes together well with the above next steps in my mind.

### Optimising the Constant (C)

When optimising the constant, it can be brought down with the following slight digit changes:

![Constant tweaking results](/constant_tweaking.png)

This creates the difference from the primes of the following:

![Constant fitting best difference](/difference_best_fit_constant.png)

### Next Steps - Optimising, and Grouping Primes

Next ideas:

- It may be valuable to tweak this `0.7964759083371821` constant in the modified equation WITHIN the groups, to find how this adjusts.
- It may also be valuable, once this is worked out, to check at the optimal fitness as the groups scale. We can break up the 1,000,000 primes into groups of 1,000 groups of 1,000.
- Modelling and plotting these might provide an interesting scale that can be seen - or some randomness that cannot be tamed. We'll see!

With optimising, there is a possibility that X, or other values, might need to be adjusted across a separate equation. First, I will try adjusting the separate constant.

### Nearness

Something I have noted when looking at the difference charts, is that **there seems to be some form of nearness bias, or gravity**. What I mean by this is that no subsequent "draw from the bag" for the next difference value is radically different from the previous difference. In fact, they seem to cluster, and follow trends, or mean-reversion. When I take the second-delta of the difference (delta of the difference), we see this chart:

![Delta of the difference values](/difference_delta.png)

(25th of October addition: I was thinking this yesterday, and forgot to write it - the above looks like a very fuzzy positive upside, with a very predictable downside variant - as it's following some sort of negative curve.)

This displays two properties:

- Positive values could skew further and further into infinity.
- Negative values seem to follow a inverse log asymptote (if that makes sense).

If we then use log scales for the Y axis of the delta difference for the first 1,000,000 primes, we see:

![1,000,000 log Y difference delta](/difference_delta_log_y.png)

And zooming into the first 250 WITHOUT log of the above series, we see:

![First 250 primes NO LOG difference delta](/250_primes_difference_delta_nolog.png)

(For this one, I recommend right-clicking, and open in another tab, as it's better to see it on a larger device.)

With LogY:

![First 250 primes log Y difference delta](/250_primes_difference_delta_log_y.png)

Both show interesting patterns, though with LogY it seems fuzzier locally, whereas at scale, it seems as if there's a bias towards negativity. The problem then might simplify into: trying to balance positive difference and negative difference at scale, which we might solve with tweaking in groups.

For the nolog plot, the bottom line of the pattern looks like a plot, as well as the band above it looks like it follows with constant fixture.

I want to remind myself and readers that **it is not my job to reason about this crazy randomness** to solve the problem - because I cannot plan the answer in any way. It's too wild. So, I will keep stepping forward cutting up little problems into heuristics, and trying to reason about the properties of what is occurring here.

### Group Fitting

Fitting to the groups is an interesting engineering problem. On one hand, we're aiming for the most optimum fitness. On the other, I am thinking this:

> Prime numbers are tricky, like their distribution. We may want to do vector analysis on the series of multivariate values, rather than simply going for the single "best" value for each group of 1,000 primes. There might be a slightly less optimal set of values which fits the group, but it might correspond to a better trend in long-term performance. We're looking for signals, not just noise - but we'll start with the best in the groups first to see if there's an interesting trend there.

As I do some testing in the background, I will call it a day. Great progress, and lots to ponder on. I want to take all of this expansive properties, and compress it down to keep it simpler.

## 25th of October, 2024 

### Refresh, Review, Reprise, Refactor?

Alright - a lot of surface area was generated yesterday by narrowing down to trying to do overfitting to the prime curve. **We know that *local optimisation* of fitness does not equate to *large optimisation* and fitness.** What does this mean then? Are we destitute to be stuck in a machine-learning-type overfitting or underfitting problem? We can't be probabalistic about this, as we're wanting to work out what's going on here. Even a *tiny* miscomputation in the algorithm equates to an enormous deviation, as per our chaos, and **three-body-problem** discussion. With this in mind, it's further clear that optimisation is not aiming to "crack the primes", it's rather an attempt to review the outputs, and see if we can understand the system, and trajectories, to build something purer, and closer to the prime curve.

I think a brief refactor may be of value - to get components into a clearer picture so I can move forward with more testing.

### Property: Negativo Contro Positivo (Negative Versus Positive)

If you look at the above **Difference Delta Between Prime Numbers And Best Fit** (24th of October), I added a note that I remembered, where it looks to have a predictable volatility: volatile upside (positive), and a predictable downside (negative). A future experiment I will do will be with the actual data points, in a more frequenty analysis format.

### Precision On Single Primes

I had an idea when looking at groups, and considering looking at smaller groups - like 100, instead of 1,000. Out of nowhere, the idea popped up:

> What if we look at the values required to get perfect precision on a single prime? In other words, let's try tuning to a group of 1, where X stays stable, and we modify Y (log of), A (coefficient of x * ... term), and B (log(x*B) coefficient).

By doing this, I quickly experimented and found there were interesting trade-offs with precision. For the first few primes - I could get good precision. With the intervals I was stepping (0.1), that quickly started to face at higher primes, like the 100th, and 1000th.

I then thought: **what if I could eliminate multivariate?** Where I began to simplify the equation by removing variations on single components.

I removed A variations - precision was still there.

I removed B variations - precision went away.

I added a higher precision to Y, and it came back. I had to move it to a `0.00001` step interval for Y, for it to return to finding a close-enough (within `0.00461` precision) result to the prime, with a Y value of `2.39269`. This is fascinating!

When I do the same for the other variables (singling them out as being adjusted), and Y=7 (how I found this expression first), the following properties occur:

(101th Prime 547)

- `A`: 0.0011 precision at A=2.283520.
- `B`: incredible precision at B=373.763.
- `Y`: 0.000735 precision at Y=2.344690.

(501st Prime: 3581)

- `A`: 0.00131 precision at A=2.237360.
- `B`: incredible precision at B=2191.157000.
- `Y`: 0.00434 precision at Y=2.386280.

(1001th Prime: 7927)

- `A`: 0.009994 precision at A=2.23048.
- `B`: 0.000691 precision at B=4920, and incredibly close precision at B=4920.007.
- `Y`: 0.004606 precision at Y=2.392690.

With this in mind, it's important to realise that we can likely fit any equation to the primes in order to manufacture a series. Though, if we were doing this trivially, we'd likely get something very close to the prime numbers in terms of relationship to the values.

With the above, we can see that the underlying equation has values which show some sort of pattern (the XX in 2.XX for A may for example be a prime number in disguise, and fool us), and we can also see B grows as the primes grow (it may grow by the prime number increase). We can also see that Y fits this above-2 value, and grows positively (from sample data so far, whereas unlike B in our sample data, which we do not know its growth yet).

Either way - we have some more exploring to do, and I'll begin to work on a precision finder for each of these value, and I feel as if this was a very interesting find for the equation. **Off to data mining so we can plot these values over the primes!**

### Shower Reflections 

<div style={{margin: "30px 0;"}}></div>

> I just had a shower (not necessary for you to know), but during the whole shower I could think of little but this problem. I spend a lot of time working in data, and there are periods of enjoyment and creativity, but truly nothing compares to the unbounded, self-fueled creation space, over anything else. I realise I enjoy, more than anything, chewing on these tough ideas that just don't give. I want to get to a place where this can be my main endeavour, as I do not even know what problems are out there to look at and help solve.

I see two current "next steps" that stand above the rest of the many ideas I am having for this prime problem:

1. Building a streamlined per-prime tester, precision finder, and data collector.
2. Investigating, with frequency analysis, the positive and negative differences with a static value over time - to see if there are relationships in the positive versus negative values, as I pointed out in the "volatile positive values, predictable bounded negative values", and seeing if there is some sort of relationship there in frequencies / samples.

Next, I will work on data mining, and move into the positive & negative potentially as well, to gather more properties.

### Building *The Prime Miner*

Some following thoughts:

- We want to do two things: find the best value to fit the equation to the prime, and we also want to capture the value range which results in an output which *rounds to the prime*. This is so when if we're fitting an equation to this, we can look to fit to the range, as well as the most precise, to see if there's a difference.
- Idea: When we find a best-fit range of values (values that would result in a value which is roundable to the prime), can we find a whole fraction which fits that value? If so, can we find a pattern in the whole fractions, or the possibility of a whole fraction-ness?

### Reviewing The Results

It's a slow process, and I've only done the bottom 1,229 so far, so I do not want to get ahead of myself. It is utterly fascinating seeing this pattern. I am going to go for a walk, let it run a bit more, ruminate, and come back to start analysing the data.

- When A lowers from its prior most optimal value, Y-optimal raises higher for it's optimal value to arive at the prime.

<div style={{margin: "20px 0;"}}></div>

## 25th of October, 2024

I've not spent a lot of time on the problem today in terms of the hard stuff, but I have been thinking about it quite a bit. The data miner has been built, and I am going to do some analysis on the data, as well as compute with the optimal values to make sure I've got the resulting values being parsed correctly.

Below is a plot of the fittest Y value to get the prime at N for the equation. As a reminder, I will include the equation which is being optimised (without the other variables) above each plot. Below is the Y-plot:

### Y-Fitting: x * log(x, Y)

![Y-fit equation](/y_fit.png)

I was going to remove the first few primes from the sample to scale it better, but I thought it would be useful to see it how I see it - like a wild plot which I can't remember the name for. `x sin(x)` (since it's sort of sinusoidal)?

With the front clipped (so you can see the smaller segment better), we get this:

![Y-fit from-1000 clipped](/y_fit_clipped_100.png)

And the first 500 primes:

![Y-fit upto-500 clipped](/y_fit_clipped_upto_500.png)

### A-Fitting: (A * x) * log(x, 7)

A-fit macro chart:

![A-fit equation](/a_fit.png)

### B-Fitting x * log(x * B, 7)

B-fit macro chart:

![B-fit equation](/b_fit.png)

### There's a lot to unpack here

The above are quite thought-provoking.

- Y-fit seems to bias negatively from the longer-term mean in the sample size.
- A-fit seems to bias positively from the longer-term mean in the sample size.
- B-fit has a positive, growthy curve, but looks light it might be slightly curving.
- I'd like to look at A+Y in a chart, to see how they behave (on initial watching, I did notice that as A increased prime-over-prime, Y tended to decrease, so this might be interesting to look at).
- Y-fit looks stable at ~2.4, but might be ever so slightly increasing.
- The further you go into the primes, the slighter the precision adjusts 
- B going up and down is quite fascinating: this is because it's known we're looking at an ever-ascending prime series, but these whole numbers for B can be better fit for future primes at a lower value. The B-fit was stepped by an interval of `1`, since it was a very high figure to reach compared to the sensitivity of A and Y. This might reveal something about the weirdness of primes: what we're observing here is that **some future primes are better fit to lower B values, although there might be some series of compatible B-values which have a more consistent looking curve - since the range is wide in B values which can predict the prime**. This needs some more thought, as I'm struggling to wrap my head around it.
- A further variable I have not controlled yet is X. I am curious how, if we keep the equation at `x * log(x, 7)`, what X values are required in order to hit the primes. This is a different way of looking at things, since it looks to a fit curve, then asks what the input is for the N. So it's more like trying to find an equation for the right X value on the curve.

Let's start by looking at the overlapping Y and A charts:

![Y & A Overlapping Marco Fitness](/y_a_fittest.png)

And clipping the fronts, and standardising the difference from the first value to start from the same spot:

![Y & A Standardised overlapping from 150](/y_a_normalised_from_150.png)

And Y + A plotted from the 100th prime:

![Y+A from the 100th prime](/y_plus_a_from_100.png)

And finally, the delta between Y and A:

![Y & A Delta](/y_a_delta.png)

A few additional thoughts:

- `A` and `Y` fitnesses seem negatively correlated, though there is a bias for `Y` fitness to slowly grow out of this correlation over the long-term in the sample we have.
- There might be a better way to understand the gap between these values, to understand their distance over time better?
- `A` fitness seems to trend negatively, while `Y` trends positively. **Remember:** these are the optimal values to get as close to the prime as possible, which could be a bad anchoring point to center on. This is because from the data mining I did, there is a range of values which the output can be rounded to get to the prime number (found using abs fitness < 0.5). I will do analysis in the future on the ranges.
- If it was a choice between trying to pinpoint `A` fitness (trending negative) and `Y` fitness (trending positive), it seems an easier problem to wrangle to-negative complexity.
- `A` must negatively be approaching some infinite bottom point, as the `(x * A)` component cannot reach 0, as it would no longer produce a positive prime number.

### Confirmation Bias

We're looking at a small sample of data. It's possible that there's a breaking point where we reach a threshhold, and things adjust. I will aim to keep researching this research space to better understand it, as it feels like the right pathway (understand what's infront of me and already difficult to compute, then go there).

### Y-Fit Deltas, and A-Fit Deltas

(The below are all from the 100th prime to be able to better visualise the pattern on a smaller figure.)

Y-fit delta between each value adjustment:

**I just had a mind-melting moment. A true "WTF?" when plotting the deltas between Y-fitness and A-fitness.**

I don't really know what to make of it yet, but this **idea of capped upside and downside** is returning.

Take a look at the rate of change (delta) for Y-fitness:

![Y-fit deltas](/y_fit_deltas.png)

And A-fit deltas:

![A-fit deltas](/a_fit_deltas.png)

And now, overlapping A and Y deltas:

![Y-fit deltas](/y_fit_delta_with_a_fit_delta.png)

I'm sure this is just bamboozling once again, but I feel as if it describes the slow tilt-up with any attempt at fitting the prime curve. We see a semi-related, semi-correlated inverse relationship, though it slightly deviates over-time. 

*(Added as a remembered previous thought: is it possible to then define the value Y or A will be, by using this positive capped value, and negative capped value?)*

This makes me want to investigate the delta of the difference between Y-A chart to see what manifests there:

![Y-A Diff Deltas](/y_a_diff_deltas.png)

I saw this here, but I did not notice it in the above charts: they seem to also have a capped upside/downside, but it tends to have more volatility also.

Zooming in, the choice of up or down seems random. I will reflect on the above for a while, but I wanted to do some frequency analysis to see if there's some pattern in the delta's movement. Easier to understand, there is:

1. The size of the move (?)
2. Whether it was a positive domain move, or a negative domain move

And, if (2) can be solved, it all helps in the grand scheme.

So, I've done some frequency analysis before I go to bed, and here is a positive value for the number it was positive, then negative for how many it was negative for:

![Y-A Difference, Delta, Frequency Analysis](/y_a_diff_delta_frequency_period.png)

And, the clipped first 75:

![Y-A Difference, Delta, Frequency Analysis upto 75](/y_a_diff_delta_frequency_period_upto_75.png)

And I leave you with the above for this day at 11:45PM. Some further frequency analysis on the pulls might be of interest - for example - by checking the frequency of 1's, 2's, etc - maybe a gaussian will emerge? I'll believe it if I see it!

Further, tomorrow, in my reflection, I want to bring this back home and ground it back in the equation, so we're able to understand where we are. I do not want to go down a rabbit hole, so it's always important to re-ground on where we are, and what we have so far.

## 27th of October, 2024

G'day. I just played Bingo for the first time at the Merimbula RSL. It was definitely an experience. In the first half, it was quite fun due to the novelty. In the second half, it steadily got tiresome mentally fixturing on sheets of paper and numbers, even as a 23 year old casual number enjoyer.

What was incredible was my friend Vanessa won, but we did not know we had to call it before the last number (which won the card) moved on. We almost won $50, which was unfortunate, but anyway - life goes on.

Anyway. I was thinking about how I was discussing the two problem spaces above (solving the direction, and amplitude, are two separate problems), and how:

- If it were possible to solve the directional frequency problem, we would accidentally discover the bounds for where the prime will fit in the equation. This is if we are doing it in an iterable way (solving Prime 1, Prime 2, etc).
- For instance: if we knew the next Y-Fit value (bounded positive side, see Y-fit delta above) is going to be positive for the next prime, then we might be able to know the bounds for where the value can lie. I don't fully understand this relationship yet, but it could be valuable.

<div style={{margin: "20px 0;"}}></div>

### Optimisation 

The next stage to me, is data collection. I want to optimise the data collector / miner, since that has proven itself a valuable component to the research. Time to supercharge the miner.

### Upcoming Travel

I am travelling from the 29th of October to Bali. So updates will be a little less frequent, but they will still occur. Over my flight, as this is an internet-less problem, I'll be able to do experiments.

### Work: Optimisation

I've written an optimised Y-fit optimizer. I've decide to set the cutoff at an absolute fitness of 0.001 for good measure.

Before, I was doing 1,000,000 tests every iteration. Now, I've tested on the 201st prime 1,227, and it took 187 steps in order to get the right Y-fit value of `2.3805784`.

This is hilariously faster (I was not trying to tweak performance), and is 5,348 times faster. The other positive to this approach is that **it does not assume a step interval**, so it will always attempt to narrow down on the correct value, no matter how deep it has to go to get there. These are two big advantages.

*(I've also made it select the just-over-prime value, rather than getting a few results below prime then rounded to prime, for consistency.)*

The downside was the engineering was hard for my mind to wrap around. I need to work slowly to get the tools I need.

I've run the fitter on ~72,000 primes (first 1,000,000), and it completed the fittest-Y within < 10 seconds (9.63s). It was expected to take ~20 hours using the prior method, just for the Y-Fit values (multiple days for Y, A and B computations).

Here is a plot of the up-to 1 million primes:

![1 million primes Y-Fitness](/y_fit_upto_1m_primes.png)

(I will fit this to a prime generator also, to extend the plot further and further out, and look for a larger prime set.)

This further provides more experimentation allowance, since I can now adjust multivariate values (Y=7 for A, for example), to see what can happen (can adjusting Y reduce volatility in A, or increase it, or nothing?), and I can also compute the fitness values much quicker.

At a minimum, I feel as if I am answering the confusing questions I had in 2022/2023 regarding **why the heck do you fit a equation to the primes at 0-10,000, for it to all fall apart**. There are still many questions and tests to do, but this is a fascinating sub-area of exploration.

### What are we fitting?

I had the thought to what if we're over-fitting the values. What if this just represents another representation for the prime in a hidden way? I think that could be a piece of it, though I tried running the Y-fitter on ascending X values, trying to fit the X output, and it's a linear curve like Y=X.

So what we're currently tweaking is:

> Assuming a linear X (1, 2, 3), what log base (Y) do we need to get the Xth prime?

<div style={{margin: "20px 0;"}}></div>

### 1m Y-Fit Primes Log

Below is a figure of the primes Y-Fit with a X-scale which is logarithm:

![1m y-fit x-scale in log](/y_fit_1m_xlog.png)

## 28th of October, 2024

Hello! I'm in Sydney at the moment, preparing to fly to Bali tomorrow with my partner.

I've plugged in the 1 billionth prime (22,801,763,489) into the tweaker for Y, and got a Y of `2.4814544815638517`. For the trillionth prime (29,996,224,275,833), the Y value is `2.5121776604449253` - cracking 2.5!

For A, on the other hand, the A value for the billionth prime is `2.141080625935799`, and trillionth is `2.112479195277878`. We're seeing Y go up, and A go down. I'm presuming we can reach infinitely large primes if we're able to crack this, but the race to infinidecimal on the other side will make it challenging.

## 29th of October, 2024

This means that:

A-fitting: negatively biased.
Y-fitting: positively biased.
Y-fitting growth overpowers A-fitting negative growth.

Per `x * log(x*B, 7)` being fit, it's quite linear, and over time gets closer to linearity. At the start it's quite volatile, and over higher N's, it begins to look linear. Here is a plot with X and Y log:

![B-fit X & Y log scale](/b_fit_x_y_log.png.png)

